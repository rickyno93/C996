{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Sung Hwan Justin Yoon  \n",
    "Course: Programming in Python- C996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Explain how the Python program extracts the web links from the HTML code of the “Current Estimates,” found in web links section.    \n",
    "\n",
    " <p>\n",
    "<font size=\"4\">\n",
    "The python program extracts web links by requesting to open an url using the urllib library. The html code from the request is parsed using BeautifulSoup. This library parses through the code and finds the reference links that are found in the websites. \n",
    "</font size>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the url and find all links\n",
    "response = urllib.request.urlopen(\"https://www.census.gov/programs-surveys/popest.html\")\n",
    "soup = BeautifulSoup(response,from_encoding=response.info().get_param('charset'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the html_code to html_code.txt\n",
    "with open('html_code.txt','w',encoding='utf-8') as html_code:\n",
    "    html_code.write(str(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\n",
      "<html lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      " <head>\n",
      "  <!--[if lt IE 9]><meta http-equiv=\"X-UA-Compatible\" content=\"IE=EmulateIE8\"  /><![endif]-->\n",
      "  <!--[if gte IE 9]><meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"> <![endif]-->\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <link href=\"/etc/designs/census/bootstrap.css\" rel=\"styleshe\n"
     ]
    }
   ],
   "source": [
    "#view the first 500 characters of the html code\n",
    "print(soup.prettify()[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Explain the criteria you used to determine if a link is a locator to another HTML page. Identify the code segment that executes this action as part of your explanation.  \n",
    "\n",
    " <p>\n",
    "<font size=\"4\">\n",
    "To determine if the link was a locator to another html page, BeautifulSoup was used to find lines with the 'a' tag (which stands for anchor/hyperlinks) and those that were \"href\" (which stands for hypertext references). \n",
    "    </font size>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list called web_list and append all links with <a> that are \"href\"\n",
    "web_list = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    web_list.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a series with the webslist \n",
    "web_series = pd.Series(data = web_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15         https://www.census.gov/topics/population.html\n",
       "51                 https://www.census.gov/EconomicCensus\n",
       "129     https://www.census.gov/programs-surveys/cps.html\n",
       "178                   /programs-surveys/popest/news.html\n",
       "59     https://www.census.gov/programs-surveys/survey...\n",
       "133    https://www.census.gov/programs-surveys/poppro...\n",
       "122    https://www.census.gov/programs-surveys/decenn...\n",
       "171                               /programs-surveys.html\n",
       "215                       https://www.census.gov/privacy\n",
       "113            https://www.census.gov/library/audio.html\n",
       "162                                             #content\n",
       "148               https://www.census.gov/about/what.html\n",
       "164                   /programs-surveys/popest/data.html\n",
       "90     https://www.census.gov/topics/population/hispa...\n",
       "106       https://www.census.gov/data/related-sites.html\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#15 weblinks found in the the scraped html\n",
    "web_series.sample(n= 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Explain how the program ensures that relative links are saved as absolute URIs in the output file. Identify the code segment that executes this action as part of your explanation.  \n",
    "\n",
    "<p>\n",
    "    <font size =\"4\">\n",
    "The program ensured that relative links were saved as absolute URL's by using regular expressions. The regular expression that was used is shown in the code below. The string that was extracted from the array of links was made to only follow a pattern of \"https://----.----.-----\" (---- symbolizing any word/charcs). This meant that anything after \".com\" or \".gov\" or any other domain name was not extracted.\n",
    "    </font size>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract using the websites that follow the regex pattern, and drop those that do not.\n",
    "unique_website = web_series.str.extract('(https://[\\w]+.[\\w]+.[\\w]+)').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Explain how the program ensures that there are no duplicated links in the output file. Identify the code that executes this action as part of your explanation.\n",
    "\n",
    "<p>\n",
    "    <font size = \"4\">\n",
    "To ensure that there were duplicates in the output file, the pandas method unique() was used. This method only gives the unique values of a pandas array. The code is shown below.\n",
    "    </font size>\n",
    "    </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique websites only.\n",
    "unique_website = unique_website[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['https://www.census.gov', 'https://www.facebook.com',\n",
       "       'https://twitter.com/uscensusbureau', 'https://www.linkedin.com',\n",
       "       'https://www.youtube.com', 'https://www.instagram.com',\n",
       "       'https://www.commerce.gov', 'https://www.usa.gov'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the unique website in an array\n",
    "#Notice how index 2 is not formatted the same as the others.   \n",
    "unique_website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change index 2 (https://twitter.com/uscensusbureau to https://twitter.com)\n",
    "unique_website[2] = 'https://twitter.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to a text file the unique websites\n",
    "with open('unique_website.txt', 'w') as file:\n",
    "    for website in unique_website:\n",
    "        file.write(\"{}\\n\".format(website))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\jshyo\\\\Desktop\\\\WGU\\\\practice'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print working directory to find the saved files\n",
    "%pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
